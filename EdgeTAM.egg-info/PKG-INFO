Metadata-Version: 2.4
Name: EdgeTAM
Version: 1.0
Summary: EdgeTAM: On-Device Track Anything Model
Home-page: https://github.com/facebookresearch/EdgeTAM
Author: Meta AI
Author-email: chongzhou1024@gmail.com
License: Apache 2.0
Requires-Python: >=3.10.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.3.1
Requires-Dist: torchvision>=0.18.1
Requires-Dist: numpy>=1.24.4
Requires-Dist: tqdm>=4.66.1
Requires-Dist: hydra-core>=1.3.2
Requires-Dist: iopath>=0.1.10
Requires-Dist: pillow>=9.4.0
Provides-Extra: notebooks
Requires-Dist: matplotlib>=3.9.1; extra == "notebooks"
Requires-Dist: jupyter>=1.0.0; extra == "notebooks"
Requires-Dist: opencv-python>=4.7.0; extra == "notebooks"
Requires-Dist: eva-decord>=0.6.1; extra == "notebooks"
Provides-Extra: dev
Requires-Dist: black==24.2.0; extra == "dev"
Requires-Dist: usort==1.0.2; extra == "dev"
Requires-Dist: ufmt==2.0.0b2; extra == "dev"
Requires-Dist: fvcore>=0.1.5.post20221221; extra == "dev"
Requires-Dist: pandas>=2.2.2; extra == "dev"
Requires-Dist: scikit-image>=0.24.0; extra == "dev"
Requires-Dist: tensorboard>=2.17.0; extra == "dev"
Requires-Dist: pycocotools>=2.0.8; extra == "dev"
Requires-Dist: tensordict>=0.5.0; extra == "dev"
Requires-Dist: opencv-python>=4.7.0; extra == "dev"
Requires-Dist: submitit>=1.5.1; extra == "dev"
Provides-Extra: gradio
Requires-Dist: gradio==4.44.0; extra == "gradio"
Requires-Dist: gradio_client==1.3.0; extra == "gradio"
Requires-Dist: gradio_image_prompter==0.1.0; extra == "gradio"
Requires-Dist: opencv-python==4.10.0.84; extra == "gradio"
Requires-Dist: moviepy==1.0.3; extra == "gradio"
Requires-Dist: pydantic==2.10.6; extra == "gradio"
Requires-Dist: timm==1.0.15; extra == "gradio"
Requires-Dist: eva-decord==0.6.1; extra == "gradio"
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# üöÄ EdgeTAM-SAMURAI: On-Device Tracking with Motion-Aware Memory

> **Fast on-device tracking that remembers everything**  
> Combining EdgeTAM's speed with SAMURAI's memory bank for next-level video object tracking. 

<div align="center">

https://github.com/user-attachments/assets/0ab449db-d42c-4c04-8c34-a25d2a33ddd2

</div>

---

## ‚ú® Features

- üöÄ 22√ó faster than SAM 2 (inherited from EdgeTAM)
- üß† Smart memory for long-term tracking
- üì± 16 FPS on iPhone 15 Pro Max
- üéØ Handles occlusions like a pro

## üìñ Overview
| Component | What It Brings |
|-----------|---------------|
| **EdgeTAM** | Mobile-optimized architecture, 22√ó speed boost, on-device efficiency |
| **SAMURAI** | Motion-aware memory bank, long-term tracking, occlusion handling |
| **Our Implementation** | Seamless integration of speed + memory for tracking |



## üõ†Ô∏è Quick Start

### Installation

```bash
# Clone the repo
git clone https://github.com/m-dear/edge_sam2_realtime.git
cd edge_sam2_realtime
# Install dependencies
pip install -e .
```

### üé¨ Real-time implementation

```python
import torch
from edge_sam2.build_sam import build_sam2_object_tracker

# Load the model
checkpoint = "./checkpoints/edgetam_samurai.pt"
model_cfg = "configs/edgetam_samurai.yaml"
predictor = build_sam2_object_tracker(
      config_file=model_cfg,
      ckpt_path=checkpoint,
      device="cuda:0",  # Use "cpu" for CPU inference
      # This implementation is for a single object
      num_objects=1, 
      verbose=False
  )
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    while True:        
        # Track object
        if is_first_frame:
            # Provide the initial bounding box to track a new object
            sam_out = self.predictor.track_new_object(img=img_rgb, box=bbox)
            is_first_frame = False
        else:
            # Track all previously identified objects in subsequent frames
            sam_out = self.predictor.track_all_objects(img=img_rgb)
```
‚ö†Ô∏è Implementation Note
Currently, only the `build_sam2_object_tracker` function implements the EdgeTAM and SAMURAI integration. Other functions may use standard SAM2 functionality.

### Demo code
```bash
cd EdgeTAM-SAMURAI
# Run the demo script
python demo/run_realtime.py
```
Use mouse to draw bounding box on the video frame to track a new object, or press `space or enter` to track all previously identified objects.


## üôè Acknowledgments
This project builds upon the work of the EdgeTAM authors at Meta Reality Labs and the SAMURAI authors. We extend their efforts by integrating EdgeTAM's speed with SAMURAI's memory bank to create a powerful on-device tracking solution.


## üìÑ Citation
```bibtex
@article{zhou2025edgetam,
  title={EdgeTAM: On-Device Track Anything Model},
  author={Zhou, Chong and Zhu, Chenchen and Xiong, Yunyang and Suri, Saksham and Xiao, Fanyi and Wu, Lemeng and Krishnamoorthi, Raghuraman and Dai, Bo and Loy, Chen Change and Chandra, Vikas and Soran, Bilge},
  journal={arXiv preprint arXiv:2501.07256},
  year={2025}
}

@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}
@misc{yang2024samurai,
  title={SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory}, 
  author={Cheng-Yen Yang and Hsiang-Wei Huang and Wenhao Chai and Zhongyu Jiang and Jenq-Neng Hwang},
  year={2024},
  eprint={2411.11922},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2411.11922}, 
}
```
## ü§ù Contributors
- **EdgeTAM** (Meta Reality Labs) for efficient on-device tracking
- **SAMURAI** for motion-aware memory mechanisms  
- **SAM 2** (Meta) for the foundational segmentation model
- **[zdata-inc](https://github.com/zdata-inc/sam2_realtime)** for real-time implementation insights

---

<div align="center">

[üåü Star us on GitHub](https://github.com/m-dear/edge_sam2_realtime.git) 

</div>
